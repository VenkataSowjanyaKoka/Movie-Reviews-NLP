{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for structured data we use Spark SQL, SparkSession acts a pipeline between data and sql statements\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparksession is like a class and we need to create an instance of a class to utilize\n",
    "spark = SparkSession.builder.appName(\"NLP_5A_Data_Processing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file data\n",
    "Movie_Reviews_DF = spark.read.csv(\"D:/Movie_reviews.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (7087, 2)\n"
     ]
    }
   ],
   "source": [
    "#Seeing the shape of the dataset\n",
    "print(\"Shape:\", (Movie_Reviews_DF.count(), len(Movie_Reviews_DF.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the schema\n",
    "#both columns are of string type\n",
    "Movie_Reviews_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+---------+\n",
      "|Review                                                                  |Sentiment|\n",
      "+------------------------------------------------------------------------+---------+\n",
      "|Harry Potter dragged Draco Malfoy ’ s trousers down past his hips and   |0        |\n",
      "|the Da Vinci Code sucked.                                               |0        |\n",
      "|Oh, and Brokeback Mountain is a TERRIBLE movie...                       |0        |\n",
      "|Brokeback Mountain is fucking horrible..                                |0        |\n",
      "|The Da Vinci Code is awesome!!                                          |1        |\n",
      "|I want to be here because I love Harry Potter, and I really want a place|1        |\n",
      "|Harry Potter dragged Draco Malfoy ’ s trousers down past his hips and   |0        |\n",
      "|As I sit here, watching the MTV Movie Awards, I am reminded of how much |0        |\n",
      "|I love Harry Potter..                                                   |1        |\n",
      "|, she helped me bobbypin my insanely cool hat to my head, and she laughe|0        |\n",
      "+------------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loding the random function\n",
    "from pyspark.sql.functions import rand \n",
    "#Displaying random observations from the data\n",
    "Movie_Reviews_DF.orderBy(rand()).show(10,False)  # Note Sentiment values are read in as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "#Filtering the data for data only with only either 0 or 1 sentiment value\n",
    "Movie_Reviews_DF = Movie_Reviews_DF.filter(((Movie_Reviews_DF.Sentiment =='1') | (Movie_Reviews_DF.Sentiment =='0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the count to see if any rows are deleted (rows with different sentiment values)\n",
    "Movie_Reviews_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0| 3081|\n",
      "|        1| 3909|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Grouping by sentiment values to see balance of data\n",
    "#(Fairly balanced)\n",
    "Movie_Reviews_DF.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the schema again\n",
    "Movie_Reviews_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to perform logistic regression \n",
    "#we should have sentiment value of numeric datatype\n",
    "#Adding a column label to store converted float values \n",
    "#from string value in Sentiment (and dropping the Sentiment(String type) column)\n",
    "Movie_Reviews_DF = Movie_Reviews_DF.withColumn(\"Label\", Movie_Reviews_DF.Sentiment.cast('float')).drop('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the schema again\n",
    "Movie_Reviews_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+-----+\n",
      "|Review                                                                      |Label|\n",
      "+----------------------------------------------------------------------------+-----+\n",
      "|I am going to start reading the Harry Potter series again because that i    |1.0  |\n",
      "|Harry Potter is AWESOME I don't care if anyone says differently!..          |1.0  |\n",
      "|I want to be here because I love Harry Potter, and I really want a place    |1.0  |\n",
      "|But if Crash won the academy award, Brokeback Mountain must have sucked     |0.0  |\n",
      "|Because I would like to make friends who like the same things I like, an    |1.0  |\n",
      "|Brokeback Mountain was an AWESOME movie.                                    |1.0  |\n",
      "|the last stand and Mission Impossible 3 both were awesome movies.           |1.0  |\n",
      "|Other than that, all I've heard is that the Da Vinci Code kinda sucks!      |0.0  |\n",
      "|\"I think the movie \"\" Brokeback Mountain \"\" was stupid and overexagerated..\"|0.0  |\n",
      "|Brokeback Mountain is fucking horrible..                                    |0.0  |\n",
      "+----------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying random data\n",
    "Movie_Reviews_DF.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 3909|\n",
      "|  0.0| 3081|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for the values after transformation\n",
    "Movie_Reviews_DF.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding length column to the dataframe\n",
    "#Length of the review might matter because repetition of words would occur in the same review\n",
    "#Loading length function \n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each row calculating length of review and adding it to a new column\n",
    "Movie_Reviews_DF = Movie_Reviews_DF.withColumn('length',length(Movie_Reviews_DF['Review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+------+\n",
      "|Review                                                                  |Label|length|\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "|I hate Harry Potter.                                                    |0.0  |20    |\n",
      "|the last stand and Mission Impossible 3 both were awesome movies.       |1.0  |65    |\n",
      "|we're gonna like watch Mission Impossible or Hoot.(                     |1.0  |51    |\n",
      "|I think I hate Harry Potter because it outshines much better reading mat|0.0  |72    |\n",
      "|I hate Harry Potter.                                                    |0.0  |20    |\n",
      "+------------------------------------------------------------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the data\n",
    "Movie_Reviews_DF.orderBy(rand()).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Label|      avg(Length)|\n",
      "+-----+-----------------+\n",
      "|  1.0|47.61882834484523|\n",
      "|  0.0|50.95845504706264|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Average length of a review for a 0 and 1 sentiment review(negative and positive)\n",
    "#Fairly close\n",
    "Movie_Reviews_DF.groupBy('Label').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "#Importing the Tokenizer function\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking review column and creating new column tokens for storing the tokens created from review column\n",
    "tokenization = Tokenizer(inputCol='Review',outputCol='Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the Tokenizer function to the dataframe\n",
    "Tokenized_DF = tokenization.transform(Movie_Reviews_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+\n",
      "|Tokens                                                                                  |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|[the, da, vinci, code, book, is, just, awesome.]                                        |\n",
      "|[this, was, the, first, clive, cussler, i've, ever, read,, but, even, books, like, rel] |\n",
      "|[i, liked, the, da, vinci, code, a, lot.]                                               |\n",
      "|[i, liked, the, da, vinci, code, a, lot.]                                               |\n",
      "|[i, liked, the, da, vinci, code, but, it, ultimatly, didn't, seem, to, hold, it's, own.]|\n",
      "|[that's, not, even, an, exaggeration, ), and, at, midnight, we, went, to, wal-mart, to] |\n",
      "|[i, loved, the, da, vinci, code,, but, now, i, want, something, better, and, different] |\n",
      "|[i, thought, da, vinci, code, was, great,, same, with, kite, runner.]                   |\n",
      "|[the, da, vinci, code, is, actually, a, good, movie...]                                 |\n",
      "|[i, thought, the, da, vinci, code, was, a, pretty, good, book.]                         |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#looking at the tokens columns\n",
    "Tokenized_DF.select('Tokens').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get a count of tokens for each row before removing the stop words\n",
    "# importing size function from sql functions\n",
    "from pyspark.sql.functions import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting all columns from dataframe and adding a new column based on no of tokens in each observation\n",
    "# Size is a sql function to count number of items in a list\n",
    "Tokenized_DF = Tokenized_DF.select('*',size('Tokens').alias('Tokens_Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+------------+\n",
      "|Tokens                                                                                  |Tokens_Count|\n",
      "+----------------------------------------------------------------------------------------+------------+\n",
      "|[the, da, vinci, code, book, is, just, awesome.]                                        |8           |\n",
      "|[this, was, the, first, clive, cussler, i've, ever, read,, but, even, books, like, rel] |14          |\n",
      "|[i, liked, the, da, vinci, code, a, lot.]                                               |8           |\n",
      "|[i, liked, the, da, vinci, code, a, lot.]                                               |8           |\n",
      "|[i, liked, the, da, vinci, code, but, it, ultimatly, didn't, seem, to, hold, it's, own.]|15          |\n",
      "|[that's, not, even, an, exaggeration, ), and, at, midnight, we, went, to, wal-mart, to] |14          |\n",
      "|[i, loved, the, da, vinci, code,, but, now, i, want, something, better, and, different] |14          |\n",
      "|[i, thought, da, vinci, code, was, great,, same, with, kite, runner.]                   |11          |\n",
      "|[the, da, vinci, code, is, actually, a, good, movie...]                                 |9           |\n",
      "|[i, thought, the, da, vinci, code, was, a, pretty, good, book.]                         |11          |\n",
      "+----------------------------------------------------------------------------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#looking at the tokens and tokens count columns\n",
    "#Tokenization converts sentences to lower case and then creates tokens\n",
    "Tokenized_DF.select('Tokens','Tokens_Count').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of stopwords\n",
    "#Importing the StopWordsRemover function\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Tokens column and creating new column Refined Tokens for storing the tokens after removal of stopwords\n",
    "stopword_removal=StopWordsRemover(inputCol='Tokens',outputCol='Refined_Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the StopWordsRemover function to the dataframe\n",
    "Refined_DF = stopword_removal.transform(Tokenized_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|Refined_Tokens                                               |\n",
      "+-------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome.]                            |\n",
      "|[first, clive, cussler, ever, read,, even, books, like, rel] |\n",
      "|[liked, da, vinci, code, lot.]                               |\n",
      "|[liked, da, vinci, code, lot.]                               |\n",
      "|[liked, da, vinci, code, ultimatly, seem, hold, own.]        |\n",
      "|[even, exaggeration, ), midnight, went, wal-mart]            |\n",
      "|[loved, da, vinci, code,, want, something, better, different]|\n",
      "|[thought, da, vinci, code, great,, kite, runner.]            |\n",
      "|[da, vinci, code, actually, good, movie...]                  |\n",
      "|[thought, da, vinci, code, pretty, good, book.]              |\n",
      "+-------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selecting only the refined tokens column which has tokens after stop words have been removed\n",
    "Refined_DF.select(['Refined_Tokens']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get a count of tokens for each row after removing the stop words\n",
    "#importing size function from sql functions\n",
    "from pyspark.sql.functions import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting all columns from dataframe and adding a new column based on no of refined tokens in each observation\n",
    "#Size is a sql function to count number of items in a list\n",
    "Refined_DF = Refined_DF.select('*',size('Refined_Tokens').alias('Refined_Tokens_Count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------+------------+-------------------------------------------------------------+--------------------+\n",
      "|Tokens                                                                                  |Tokens_Count|Refined_Tokens                                               |Refined_Tokens_Count|\n",
      "+----------------------------------------------------------------------------------------+------------+-------------------------------------------------------------+--------------------+\n",
      "|[the, da, vinci, code, book, is, just, awesome.]                                        |8           |[da, vinci, code, book, awesome.]                            |5                   |\n",
      "|[this, was, the, first, clive, cussler, i've, ever, read,, but, even, books, like, rel] |14          |[first, clive, cussler, ever, read,, even, books, like, rel] |9                   |\n",
      "|[i, liked, the, da, vinci, code, a, lot.]                                               |8           |[liked, da, vinci, code, lot.]                               |5                   |\n",
      "|[i, liked, the, da, vinci, code, a, lot.]                                               |8           |[liked, da, vinci, code, lot.]                               |5                   |\n",
      "|[i, liked, the, da, vinci, code, but, it, ultimatly, didn't, seem, to, hold, it's, own.]|15          |[liked, da, vinci, code, ultimatly, seem, hold, own.]        |8                   |\n",
      "|[that's, not, even, an, exaggeration, ), and, at, midnight, we, went, to, wal-mart, to] |14          |[even, exaggeration, ), midnight, went, wal-mart]            |6                   |\n",
      "|[i, loved, the, da, vinci, code,, but, now, i, want, something, better, and, different] |14          |[loved, da, vinci, code,, want, something, better, different]|8                   |\n",
      "|[i, thought, da, vinci, code, was, great,, same, with, kite, runner.]                   |11          |[thought, da, vinci, code, great,, kite, runner.]            |7                   |\n",
      "|[the, da, vinci, code, is, actually, a, good, movie...]                                 |9           |[da, vinci, code, actually, good, movie...]                  |6                   |\n",
      "|[i, thought, the, da, vinci, code, was, a, pretty, good, book.]                         |11          |[thought, da, vinci, code, pretty, good, book.]              |7                   |\n",
      "+----------------------------------------------------------------------------------------+------------+-------------------------------------------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the tokens,tokens count and refined tokens, refined tokens count columns\n",
    "#To see if the counts vary which indicates removal of stop words in tokens \n",
    "Refined_DF.select('Tokens','Tokens_Count','Refined_Tokens','Refined_Tokens_Count').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+------------+-------------------------------------------------+--------------------+\n",
      "|Review                                                                  |Label|length|Tokens                                                                                   |Tokens_Count|Refined_Tokens                                   |Refined_Tokens_Count|\n",
      "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+------------+-------------------------------------------------+--------------------+\n",
      "|My dad's being stupid about brokeback mountain...                       |0.0  |49    |[my, dad's, being, stupid, about, brokeback, mountain...]                                |7           |[dad's, stupid, brokeback, mountain...]          |4                   |\n",
      "|Brokeback Mountain was an AWESOME movie.                                |1.0  |40    |[brokeback, mountain, was, an, awesome, movie.]                                          |6           |[brokeback, mountain, awesome, movie.]           |4                   |\n",
      "|Brokeback Mountain was a shitty movie.                                  |0.0  |38    |[brokeback, mountain, was, a, shitty, movie.]                                            |6           |[brokeback, mountain, shitty, movie.]            |4                   |\n",
      "|I want to be here because I love Harry Potter, and I really want a place|1.0  |72    |[i, want, to, be, here, because, i, love, harry, potter,, and, i, really, want, a, place]|16          |[want, love, harry, potter,, really, want, place]|7                   |\n",
      "+------------------------------------------------------------------------+-----+------+-----------------------------------------------------------------------------------------+------------+-------------------------------------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at random data\n",
    "Refined_DF.orderBy(rand()).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Tokens_Count: integer (nullable = false)\n",
      " |-- Refined_Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Refined_Tokens_Count: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at schema\n",
    "Refined_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating features based on Count Vectorization in PySpark using the Refined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprting function for CV calculation\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking refined tokens column and creating new column CV features \n",
    "count_vec=CountVectorizer(inputCol='Refined_Tokens', outputCol='CV_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_DF=count_vec.fit(Refined_DF).transform(Refined_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+----------------------------------------------------------------------------------+\n",
      "|Refined_Tokens                                               |CV_features                                                                       |\n",
      "+-------------------------------------------------------------+----------------------------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome.]                            |(2302,[0,1,4,43,236],[1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|[first, clive, cussler, ever, read,, even, books, like, rel] |(2302,[11,51,229,237,275,742,824,1087,1250],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[liked, da, vinci, code, lot.]                               |(2302,[0,1,4,53,356],[1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|[liked, da, vinci, code, lot.]                               |(2302,[0,1,4,53,356],[1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|[liked, da, vinci, code, ultimatly, seem, hold, own.]        |(2302,[0,1,4,53,655,1339,1427,1449],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n",
      "|[even, exaggeration, ), midnight, went, wal-mart]            |(2302,[46,229,271,1150,1990,2203],[1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "|[loved, da, vinci, code,, want, something, better, different]|(2302,[0,1,22,30,111,219,389,535],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
      "|[thought, da, vinci, code, great,, kite, runner.]            |(2302,[0,1,4,228,1258,1716,2263],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |\n",
      "|[da, vinci, code, actually, good, movie...]                  |(2302,[0,1,4,33,226,258],[1.0,1.0,1.0,1.0,1.0,1.0])                               |\n",
      "|[thought, da, vinci, code, pretty, good, book.]              |(2302,[0,1,4,223,226,228,262],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "+-------------------------------------------------------------+----------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_DF.select(['Refined_Tokens','CV_features']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method takes each word in the BoW and counts how many times that word appears in each document. It is basically computing Term Frequency (TF) or the number of times each word occurs in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_DF_Model = CV_DF.select(['CV_features','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------+\n",
      "|Label|CV_features                                                                       |\n",
      "+-----+----------------------------------------------------------------------------------+\n",
      "|1.0  |(2302,[0,1,4,43,236],[1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|1.0  |(2302,[11,51,229,237,275,742,824,1087,1250],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|1.0  |(2302,[0,1,4,53,356],[1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|1.0  |(2302,[0,1,4,53,356],[1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|1.0  |(2302,[0,1,4,53,655,1339,1427,1449],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |\n",
      "|1.0  |(2302,[46,229,271,1150,1990,2203],[1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "|1.0  |(2302,[0,1,22,30,111,219,389,535],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |\n",
      "|1.0  |(2302,[0,1,4,228,1258,1716,2263],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |\n",
      "|1.0  |(2302,[0,1,4,33,226,258],[1.0,1.0,1.0,1.0,1.0,1.0])                               |\n",
      "|1.0  |(2302,[0,1,4,223,226,228,262],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "+-----+----------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_DF_Model.select(['Label','CV_features']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CV_features: vector (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To see the schema of the dataset\n",
    "CV_DF_Model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data of CV model\n",
    "CV_Training_DF,CV_Test_DF = CV_DF_Model.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2921|\n",
      "|  0.0| 2312|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the balance of training dataframe of CV model\n",
    "CV_Training_DF.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  988|\n",
      "|  0.0|  769|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the balance of testing dataframe of CV model\n",
    "CV_Test_DF.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency(TF) and Inverse Document Frequency(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating features based on TF-IDF in PySpark using the Refined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprting function for TF and IDF calculation\n",
    "from pyspark.ml.feature import HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TERM FREQUENCY\n",
    "#It is the score based on the number of times the word appears in current dataframe\n",
    "#Taking refined tokens column and creating new column tf features for storing the tf value created\n",
    "hashing_vec=HashingTF(inputCol='Refined_Tokens',outputCol='TF_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the HashingTF function to the dataframe\n",
    "Hashing_DF = hashing_vec.transform(Refined_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "|Refined_Tokens                                              |TF_features                                                                                                  |\n",
      "+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome.]                           |(262144,[93284,111793,189113,212976,235054],[1.0,1.0,1.0,1.0,1.0])                                           |\n",
      "|[first, clive, cussler, ever, read,, even, books, like, rel]|(262144,[47372,82111,113624,120246,139559,174966,203802,208258,227467],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[liked, da, vinci, code, lot.]                              |(262144,[32675,93284,111793,227152,235054],[1.0,1.0,1.0,1.0,1.0])                                            |\n",
      "|[liked, da, vinci, code, lot.]                              |(262144,[32675,93284,111793,227152,235054],[1.0,1.0,1.0,1.0,1.0])                                            |\n",
      "+------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the refined tokens and corresoponding TF features columns\n",
    "Hashing_DF.select(['Refined_Tokens','TF_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The term TF represents term frequency and computes a ratio of the number of times a word appears in a document divided by the total number of terms in that document. It therefore attempts to measure “importance” – higher the value, the more frequently the term was used with respect to other terms in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[da, vinci, code, book, awesome.]---(262144,[93284,111793,189113,212976,235054],[1.0,1.0,1.0,1.0,1.0])  \n",
    "#262144 - Total number of tokens in the dataframe\n",
    "#93284 - frequency of the word da\n",
    "#111793 - frequency of the word vinci\n",
    "#189113 - frequency of the word code\n",
    "#212976 - frequency of the word book\n",
    "#235054 - frequency of the word awesome\n",
    "#[1.0,1.0,1.0,1.0,1.0])-- list indicating the presence of the words [da, vinci, code, book, awesome.] in the review with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INVERSE DOCUMENT FREQUENCY\n",
    "#It is calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm\n",
    "#Taking TF features column and creating new column TF-IDF features for storing the TF-IDF value created\n",
    "TF_IDF_vec=IDF(inputCol='TF_features',outputCol='TF_IDF_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The term IDF, computes a measure of relative importance of the term with respect to the same term used in all other documents in the corpus. Thus, if a term appears in all documents, it’s not helping in differentiating documents. Such a term will therefore be assigned a very low relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the IDF function to the dataframe\n",
    "TF_IDF_DF = TF_IDF_vec.fit(Hashing_DF).transform(Hashing_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Refined_Tokens                                               |TF_IDF_features                                                                                                                                                                                                                           |\n",
      "+-------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome.]                            |(262144,[93284,111793,189113,212976,235054],[1.469010739519602,1.2610218398134343,6.079790164272204,4.0401945311395675,1.2620319409094194])                                                                                               |\n",
      "|[first, clive, cussler, ever, read,, even, books, like, rel] |(262144,[47372,82111,113624,120246,139559,174966,203802,208258,227467],[6.144328685409775,4.247208700523894,7.7537665978438755,8.15923170595204,8.15923170595204,5.716884670582836,6.772937344832149,2.7039105905943384,8.15923170595204])|\n",
      "|[liked, da, vinci, code, lot.]                               |(262144,[32675,93284,111793,227152,235054],[4.267411407841413,1.469010739519602,1.2610218398134343,7.242940974077885,1.2620319409094194])                                                                                                 |\n",
      "|[liked, da, vinci, code, lot.]                               |(262144,[32675,93284,111793,227152,235054],[4.267411407841413,1.469010739519602,1.2610218398134343,7.242940974077885,1.2620319409094194])                                                                                                 |\n",
      "|[liked, da, vinci, code, ultimatly, seem, hold, own.]        |(262144,[5765,32675,93284,111793,178453,193996,235054,237388],[7.7537665978438755,4.267411407841413,1.469010739519602,1.2610218398134343,8.15923170595204,8.15923170595204,1.2620319409094194,8.15923170595204])                          |\n",
      "|[even, exaggeration, ), midnight, went, wal-mart]            |(262144,[105591,146139,174966,197340,243418,248625],[8.15923170595204,4.151898520719569,5.716884670582836,6.772937344832149,8.15923170595204,8.15923170595204])                                                                           |\n",
      "|[loved, da, vinci, code,, want, something, better, different]|(262144,[33933,111793,115917,173297,179666,190256,224769,235054],[3.3229497990005616,1.2610218398134343,4.433538278715387,7.7537665978438755,5.326018361895824,4.309084104241982,7.7537665978438755,1.2620319409094194])                  |\n",
      "|[thought, da, vinci, code, great,, kite, runner.]            |(262144,[2000,33552,37254,93284,111793,235054,242361],[8.15923170595204,8.15923170595204,8.15923170595204,1.469010739519602,1.2610218398134343,1.2620319409094194,5.716884670582836])                                                     |\n",
      "|[da, vinci, code, actually, good, movie...]                  |(262144,[93284,111793,113432,132975,171076,235054],[1.469010739519602,1.2610218398134343,5.761336433153669,6.5497937935179396,3.710715330009325,1.2620319409094194])                                                                      |\n",
      "|[thought, da, vinci, code, pretty, good, book.]              |(262144,[23661,93284,111793,113432,175449,235054,242361],[6.5497937935179396,1.469010739519602,1.2610218398134343,5.761336433153669,5.485083056525511,1.2620319409094194,5.716884670582836])                                              |\n",
      "+-------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the refined tokens and corresoponding TF-IDF features columns\n",
    "#Multiplying these TF and IDF results in the TF-IDF score of a word in a document. \n",
    "#The higher the score, the more relevant that word is in that particular document.\n",
    "TF_IDF_DF.select(['Refined_Tokens','TF_IDF_features']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The product of TF and IDF gives us the TL-IDF score or weight that ranks each term by its relative importance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_DF_Model = TF_IDF_DF.select(['TF_IDF_features','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Label|TF_IDF_features                                                                                                                                                                                                                           |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |(262144,[93284,111793,189113,212976,235054],[1.469010739519602,1.2610218398134343,6.079790164272204,4.0401945311395675,1.2620319409094194])                                                                                               |\n",
      "|1.0  |(262144,[47372,82111,113624,120246,139559,174966,203802,208258,227467],[6.144328685409775,4.247208700523894,7.7537665978438755,8.15923170595204,8.15923170595204,5.716884670582836,6.772937344832149,2.7039105905943384,8.15923170595204])|\n",
      "|1.0  |(262144,[32675,93284,111793,227152,235054],[4.267411407841413,1.469010739519602,1.2610218398134343,7.242940974077885,1.2620319409094194])                                                                                                 |\n",
      "|1.0  |(262144,[32675,93284,111793,227152,235054],[4.267411407841413,1.469010739519602,1.2610218398134343,7.242940974077885,1.2620319409094194])                                                                                                 |\n",
      "|1.0  |(262144,[5765,32675,93284,111793,178453,193996,235054,237388],[7.7537665978438755,4.267411407841413,1.469010739519602,1.2610218398134343,8.15923170595204,8.15923170595204,1.2620319409094194,8.15923170595204])                          |\n",
      "|1.0  |(262144,[105591,146139,174966,197340,243418,248625],[8.15923170595204,4.151898520719569,5.716884670582836,6.772937344832149,8.15923170595204,8.15923170595204])                                                                           |\n",
      "|1.0  |(262144,[33933,111793,115917,173297,179666,190256,224769,235054],[3.3229497990005616,1.2610218398134343,4.433538278715387,7.7537665978438755,5.326018361895824,4.309084104241982,7.7537665978438755,1.2620319409094194])                  |\n",
      "|1.0  |(262144,[2000,33552,37254,93284,111793,235054,242361],[8.15923170595204,8.15923170595204,8.15923170595204,1.469010739519602,1.2610218398134343,1.2620319409094194,5.716884670582836])                                                     |\n",
      "|1.0  |(262144,[93284,111793,113432,132975,171076,235054],[1.469010739519602,1.2610218398134343,5.761336433153669,6.5497937935179396,3.710715330009325,1.2620319409094194])                                                                      |\n",
      "|1.0  |(262144,[23661,93284,111793,113432,175449,235054,242361],[6.5497937935179396,1.469010739519602,1.2610218398134343,5.761336433153669,5.485083056525511,1.2620319409094194,5.716884670582836])                                              |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_DF_Model.select(['Label','TF_IDF_features']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TF_IDF_features: vector (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To see the schema of the dataset\n",
    "TF_IDF_DF_Model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data of TFIDF model\n",
    "TFIDF_Training_DF,TFIDF_Test_DF = TF_IDF_DF_Model.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2937|\n",
      "|  0.0| 2313|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the balance of training dataframe\n",
    "TFIDF_Training_DF.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  972|\n",
      "|  0.0|  768|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the balance of testing dataframe\n",
    "TFIDF_Test_DF.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model(Using CV)\n",
    "CV_log_reg=LogisticRegression(featuresCol='CV_features',labelCol='Label').fit(CV_Training_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Training Summary(Using CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC:0.9999984452165384\n",
      "Weighted Accuracy:1.0\n",
      "Weighted Recall:1.0\n",
      "Weighted Precision:1.0\n",
      "Weighted F1 Measure:1.0\n"
     ]
    }
   ],
   "source": [
    "CV_training_summary = CV_log_reg.summary\n",
    "print(\"Area Under ROC:\" + str(CV_training_summary.areaUnderROC))\n",
    "print(\"Weighted Accuracy:\" + str(CV_training_summary.accuracy))\n",
    "print(\"Weighted Recall:\" + str(CV_training_summary.weightedRecall))\n",
    "print(\"Weighted Precision:\" + str(CV_training_summary.weightedPrecision))\n",
    "print(\"Weighted F1 Measure:\" + str(CV_training_summary.weightedFMeasure()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of test data (Using CV)\n",
    "CV_results=CV_log_reg.evaluate(CV_Test_DF).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+-----+----------------------------------------+------------------------------------------+----------+\n",
      "|CV_features                                                                          |Label|rawPrediction                           |probability                               |prediction|\n",
      "+-------------------------------------------------------------------------------------+-----+----------------------------------------+------------------------------------------+----------+\n",
      "|(2302,[0,1,4,5,12,305,340],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                            |1.0  |[-26.653836401563705,26.653836401563705]|[2.656965585654917E-12,0.999999999997343] |1.0       |\n",
      "|(2302,[0,1,4,5,64,2029],[1.0,1.0,1.0,1.0,1.0,1.0])                                   |1.0  |[-16.511341767538717,16.511341767538717]|[6.748625867178597E-8,0.9999999325137413] |1.0       |\n",
      "|(2302,[0,1,4,5,220,247,338,636,1706],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])          |1.0  |[2.6182268092636036,-2.6182268092636036]|[0.9320254537061838,0.06797454629381615]  |0.0       |\n",
      "|(2302,[0,1,4,5,303],[1.0,1.0,1.0,1.0,1.0])                                           |1.0  |[-25.349099013548834,25.349099013548834]|[9.79549026155353E-12,0.9999999999902045] |1.0       |\n",
      "|(2302,[0,1,4,5,449],[1.0,1.0,1.0,1.0,1.0])                                           |1.0  |[-20.59352738933587,20.59352738933587]  |[1.1385305576935607E-9,0.9999999988614694]|1.0       |\n",
      "|(2302,[0,1,4,5,652],[1.0,1.0,1.0,1.0,1.0])                                           |1.0  |[-14.000816360022458,14.000816360022458]|[8.308494789964832E-7,0.999999169150521]  |1.0       |\n",
      "|(2302,[0,1,4,10,14,100,524,1294,1957,2049],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|0.0  |[31.880716800010358,-31.880716800010358]|[0.9999999999999858,1.426857540299771E-14]|0.0       |\n",
      "|(2302,[0,1,4,12,16,236,238,247],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |1.0  |[-14.595468879956611,14.595468879956611]|[4.5842490699912783E-7,0.999999541575093] |1.0       |\n",
      "|(2302,[0,1,4,12,25,53,223,464,956],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])            |1.0  |[-42.80294422752487,42.80294422752487]  |[2.575831872851638E-19,1.0]               |1.0       |\n",
      "|(2302,[0,1,4,12,33],[1.0,1.0,1.0,1.0,1.0])                                           |1.0  |[-21.07250649368786,21.07250649368786]  |[7.052233925854608E-10,0.9999999992947766]|1.0       |\n",
      "+-------------------------------------------------------------------------------------+-----+----------------------------------------+------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the results of TFIDF\n",
    "CV_results.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|Label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|1.0  |1.0       |[2.656965585654917E-12,0.999999999997343] |\n",
      "|1.0  |1.0       |[6.748625867178597E-8,0.9999999325137413] |\n",
      "|1.0  |0.0       |[0.9320254537061838,0.06797454629381615]  |\n",
      "|1.0  |1.0       |[9.79549026155353E-12,0.9999999999902045] |\n",
      "|1.0  |1.0       |[1.1385305576935607E-9,0.9999999988614694]|\n",
      "|1.0  |1.0       |[8.308494789964832E-7,0.999999169150521]  |\n",
      "|0.0  |0.0       |[0.9999999999999858,1.426857540299771E-14]|\n",
      "|1.0  |1.0       |[4.5842490699912783E-7,0.999999541575093] |\n",
      "|1.0  |1.0       |[2.575831872851638E-19,1.0]               |\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "|1.0  |1.0       |[7.052233925854608E-10,0.9999999992947766]|\n",
      "+-----+----------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_results.select('Label', 'prediction','probability').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for CV results\n",
    "CV_true_positives = CV_results[(CV_results.Label == 1) & (CV_results.prediction == 1)].count()\n",
    "CV_true_negatives = CV_results[(CV_results.Label == 0) & (CV_results.prediction == 0)].count()\n",
    "CV_false_positives = CV_results[(CV_results.Label == 0) & (CV_results.prediction == 1)].count()\n",
    "CV_false_negatives = CV_results[(CV_results.Label == 1) & (CV_results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_true_postives ARE : 978 CV_true_negatives ARE : 743\n",
      "CV_false_postives ARE : 26 CV_false_negatives ARE : 10\n"
     ]
    }
   ],
   "source": [
    "#Displaying Confurion matrix of CV\n",
    "print(\"CV_true_postives ARE :\", CV_true_positives , \"CV_true_negatives ARE :\", CV_true_negatives)\n",
    "print(\"CV_false_postives ARE :\" ,CV_false_positives , \"CV_false_negatives ARE :\" ,CV_false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Recall Value is : 0.9898785425101214\n"
     ]
    }
   ],
   "source": [
    "#CV Recall Value\n",
    "CV_recall = float(CV_true_positives)/(CV_true_positives + CV_false_negatives)\n",
    "print(\"CV Recall Value is :\" ,CV_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Precision Value is : 0.9741035856573705\n"
     ]
    }
   ],
   "source": [
    "#CV Precision Value\n",
    "CV_precision = float(CV_true_positives) / (CV_true_positives + CV_false_positives)\n",
    "print(\"CV Precision Value is :\" ,CV_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cv Accuracy Value is : 0.9795105293113261\n"
     ]
    }
   ],
   "source": [
    "#CV Accuracy Value\n",
    "CV_accuracy=float((CV_true_positives + CV_true_negatives) /(CV_results.count()))\n",
    "print(\"Cv Accuracy Value is :\" ,CV_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model(Using TF-IDF)\n",
    "TFIDF_log_reg=LogisticRegression(featuresCol='TF_IDF_features',labelCol='Label').fit(TFIDF_Training_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Training Summary(Using TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC:0.9999981599465707\n",
      "Weighted Accuracy:1.0\n",
      "Weighted Recall:1.0\n",
      "Weighted Precision:1.0\n",
      "Weighted F1 Measure:1.0\n"
     ]
    }
   ],
   "source": [
    "TFIDF_training_summary = TFIDF_log_reg.summary\n",
    "print(\"Area Under ROC:\" + str(TFIDF_training_summary.areaUnderROC))\n",
    "print(\"Weighted Accuracy:\" + str(TFIDF_training_summary.accuracy))\n",
    "print(\"Weighted Recall:\" + str(TFIDF_training_summary.weightedRecall))\n",
    "print(\"Weighted Precision:\" + str(TFIDF_training_summary.weightedPrecision))\n",
    "print(\"Weighted F1 Measure:\" + str(TFIDF_training_summary.weightedFMeasure()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of test data (Using TF-IDF)\n",
    "TFIDF_results=TFIDF_log_reg.evaluate(TFIDF_Test_DF).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------+-----+----------------------------------------+------------------------------------------+----------+\n",
      "|TF_IDF_features                                                                                           |Label|rawPrediction                           |probability                               |prediction|\n",
      "+----------------------------------------------------------------------------------------------------------+-----+----------------------------------------+------------------------------------------+----------+\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "|(262144,[14,535,31179,197995],[3.203404648350779,1.8474968967991254,4.398031590258477,1.3535091525350544])|0.0  |[20.055594793030522,-20.055594793030522]|[0.9999999980503087,1.9496912797521345E-9]|0.0       |\n",
      "+----------------------------------------------------------------------------------------------------------+-----+----------------------------------------+------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the results of TFIDF\n",
    "TFIDF_results.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------------------------------+\n",
      "|Label|prediction|probability                                |\n",
      "+-----+----------+-------------------------------------------+\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |0.0       |[0.9999999980503087,1.9496912797521345E-9] |\n",
      "|0.0  |1.0       |[0.4794483458035445,0.5205516541964554]    |\n",
      "|1.0  |1.0       |[2.7314263108247142E-11,0.9999999999726856]|\n",
      "+-----+----------+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TFIDF_results.select('Label', 'prediction','probability').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For TF-IDF results BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for TFIDF results\n",
    "TFIDF_true_positives = TFIDF_results[(TFIDF_results.Label == 1) & (TFIDF_results.prediction == 1)].count()\n",
    "TFIDF_true_negatives = TFIDF_results[(TFIDF_results.Label == 0) & (TFIDF_results.prediction == 0)].count()\n",
    "TFIDF_false_positives = TFIDF_results[(TFIDF_results.Label == 0) & (TFIDF_results.prediction == 1)].count()\n",
    "TFIDF_false_negatives = TFIDF_results[(TFIDF_results.Label == 1) & (TFIDF_results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF_true_postives ARE : 960  and TFIDF_true_negatives ARE : 738\n",
      "TFIDF_false_postives ARE : 30  and TFIDF_false_negatives ARE : 12\n"
     ]
    }
   ],
   "source": [
    "#Displaying Confurion matrix of TFIDF\n",
    "print(\"TFIDF_true_postives ARE :\", TFIDF_true_positives , \" and TFIDF_true_negatives ARE :\", TFIDF_true_negatives)\n",
    "print(\"TFIDF_false_postives ARE :\" ,TFIDF_false_positives , \" and TFIDF_false_negatives ARE :\" ,TFIDF_false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Recall Value is : 0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "#TFIDF Recall Value\n",
    "TFIDF_recall = float(TFIDF_true_positives)/(TFIDF_true_positives + TFIDF_false_negatives)\n",
    "print(\"TFIDF Recall Value is :\" ,TFIDF_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Precision Value is : 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "#TFIDF Precision Value\n",
    "TFIDF_precision = float(TFIDF_true_positives) / (TFIDF_true_positives + TFIDF_false_positives)\n",
    "print(\"TFIDF Precision Value is :\" ,TFIDF_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Accuracy Value is : 0.9758620689655172\n"
     ]
    }
   ],
   "source": [
    "#TFIDF Accuracy Value\n",
    "TFIDF_accuracy=float((TFIDF_true_positives+TFIDF_true_negatives) /(TFIDF_results.count()))\n",
    "print(\"TFIDF Accuracy Value is :\" ,TFIDF_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision is about the number of actual positive cases out of all the positive\n",
    "#cases predicted by the model\n",
    "#CV Precision Value is : 0.9741035856573705(97%)\n",
    "#TFIDF Precision Value is : 0.9696969696969697(96%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall:\n",
    "#It talks about the quality of the machine learning model when it comes\n",
    "#to predicting a positive class. So out of total positive classes, how many\n",
    "#was the model able to predict correctly? This metric is widely used as\n",
    "#evaluation criteria for classification models.\n",
    "#CV Recall Value is : 0.9898785425101214(98%)\n",
    "#TFIDF Recall Value is : 0.9876543209876543(98%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Accuracy Value is : 0.9758620689655172(97%)\n",
    "#Cv Accuracy Value is : 0.9795105293113261(97%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even if the accuracy is wrong we can see the models are equally performing well by precision and recall\n",
    "#Both method are equally accurate\n",
    "#Where as using TFIDF has a better precision\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
